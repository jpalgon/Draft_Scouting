{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09249e6b",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404834e",
   "metadata": {},
   "source": [
    "Get players and picks for all drafts from 2005-2022 to add to combine data and college stats data from my previous project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd738b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6098cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_data(table_rows):\n",
    "    \"\"\"\n",
    "    Extract and return the the desired information from the td elements within\n",
    "    the table rows.\n",
    "    \"\"\"\n",
    "    # create the empty list to store the player data\n",
    "    player_data = []\n",
    "    \n",
    "    for row in table_rows:  # for each row do the following\n",
    "\n",
    "        # Get the text for each table data (td) element in the row\n",
    "        # Some player names end with ' HOF', if they do, get the text excluding\n",
    "        # those last 4 characters,\n",
    "        # otherwise get all the text data from the table data\n",
    "#         player_list = [td.get_text()[:-4] if td.get_text().endswith(\" HOF\") \n",
    "#                        else td.get_text() for td in row.find_all(\"td\")]\n",
    "        player_list = [td.get_text() for td in row.find_all(\"th\")]\n",
    "        player_list.extend([td.get_text() for td in row.find_all(\"td\")])\n",
    "        # there are some empty table rows, which are the repeated \n",
    "        # column headers in the table\n",
    "        # we skip over those rows and and continue the for loop\n",
    "        if not player_list:\n",
    "            continue\n",
    "\n",
    "        # Extracting the player links\n",
    "        # Instead of a list we create a dictionary, this way we can easily\n",
    "        # match the player name with their pfr url\n",
    "        # For all \"a\" elements in the row, get the text\n",
    "        # NOTE: Same \" HOF\" text issue as the player_list above\n",
    "        links_dict = {(link.get_text()[:-4]   # exclude the last 4 characters\n",
    "                       if link.get_text().endswith(\" HOF\")  # if they are \" HOF\"\n",
    "                       # else get all text, set thet as the dictionary key \n",
    "                       # and set the url as the value\n",
    "                       else link.get_text()) : link[\"href\"] \n",
    "                       for link in row.find_all(\"a\", href=True)}\n",
    "\n",
    "        # The data we want from the dictionary can be extracted using the\n",
    "        # player's name, which returns us their pfr url, and \"College Stats\"\n",
    "        # which returns us their college stats page\n",
    "    \n",
    "        # add the link associated to the player's pro-football-reference page, \n",
    "        # or en empty string if there is no link\n",
    "        player_list.append(links_dict.get(player_list[3], \"\"))\n",
    "\n",
    "        # add the link for the player's college stats or an empty string\n",
    "        # if ther is no link\n",
    "        player_list.append(links_dict.get(\"College Stats\", \"\"))\n",
    "\n",
    "        # Now append the data to list of data\n",
    "        player_data.append(player_list)\n",
    "        \n",
    "    return player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fc4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list that will contain all the dataframes\n",
    "# (one dataframe for each draft)\n",
    "draft_dfs_list = []\n",
    "\n",
    "# a list to store any errors that may come up while scraping\n",
    "errors_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1dd6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The url template that we pass in the draft year inro\n",
    "url_template = \"http://www.pro-football-reference.com/years/{year}/draft.htm\"\n",
    "\n",
    "# for each year from 2005 to (and including) 2022\n",
    "for year in range(2005, 2023): \n",
    "    \n",
    "    # Use try/except block to catch and inspect any urls that cause an error\n",
    "    try:\n",
    "        \n",
    "        sleep(3.15)\n",
    "        # get the draft url\n",
    "        url = url_template.format(year=year)\n",
    "\n",
    "        # get the html\n",
    "        html = urlopen(url)\n",
    "\n",
    "        # create the BeautifulSoup object\n",
    "        soup = BeautifulSoup(html, \"lxml\") \n",
    "\n",
    "        # get the column headers\n",
    "        column_headers = [th.getText() for th in \n",
    "                          soup.findAll('tr', limit=2)[1].findAll('th')]\n",
    "        column_headers.extend([\"Player_NFL_Link\", \"Player_NCAA_Link\"])\n",
    "\n",
    "        # select the data from the table using the '#drafts tr' CSS selector\n",
    "        table_rows = soup.select(\"#drafts tr\")[2:] \n",
    "\n",
    "        # extract the player data from the table rows\n",
    "        player_data = extract_player_data(table_rows)\n",
    "\n",
    "        # create the dataframe for the current years draft\n",
    "        year_df = pd.DataFrame(player_data, columns=column_headers)\n",
    "\n",
    "        # add the year of the draft to the dataframe\n",
    "        year_df.insert(0, \"Draft_Yr\", year)\n",
    "\n",
    "        # append the current dataframe to the list of dataframes\n",
    "        draft_dfs_list.append(year_df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Store the url and the error it causes in a list\n",
    "        error =[url, e] \n",
    "        # then append it to the list of errors\n",
    "        errors_list.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1816124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all drafts in one DataFrame\n",
    "draft_df = pd.concat(draft_dfs_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ca2e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select on the columns I need from the all drafts DataFrame\n",
    "drafts_df = draft_df[['Draft_Yr','Rnd','Pick','Tm','Player','Pos','Age','To','College/Univ','Player_NFL_Link','Player_NCAA_Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6b921e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if there were any errors\n",
    "errors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a31e38",
   "metadata": {},
   "source": [
    "### Save scraped data to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce384bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drafts0522 = drafts_df[drafts_df.Pos != 'Pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf62785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drafts0522.to_parquet('../Data/draft0522.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
